{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loading data from: /Users/sohammandal/Developer/trading-system/data/historical_stock_data_15min_1year.csv\n",
      "âœ… Scaler saved successfully!\n",
      "âœ… Training Data: (521468, 13), Testing Data: (130394, 13)\n",
      "ðŸ“Š **Model Accuracy:** 0.5224\n",
      "âœ… Model saved successfully at: /Users/sohammandal/Developer/trading-system/models/xgboost_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.trend import MACD, SMAIndicator\n",
    "from ta.volatility import BollingerBands\n",
    "import joblib\n",
    "\n",
    "# ðŸ“‚ Load Data\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "data_path = os.path.join(project_root, \"data\", \"historical_stock_data_15min_1year.csv\")\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\"âŒ Error: {data_path} not found. Ensure fetch script has been run.\")\n",
    "\n",
    "print(f\"âœ… Loading data from: {data_path}\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# ðŸ•’ Ensure proper datetime format & sorting\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.sort_values(by=[\"symbol\", \"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "# ðŸ”¹ Feature Engineering: Adding Technical Indicators\n",
    "def add_indicators(df):\n",
    "    df[\"returns\"] = df[\"close\"].pct_change()  # Price % Change\n",
    "\n",
    "    # ðŸŸ¢ Relative Strength Index (RSI)\n",
    "    df[\"rsi\"] = df.groupby(\"symbol\", group_keys=False)[\"close\"].apply(lambda x: RSIIndicator(x, window=14).rsi()).reset_index(level=0, drop=True)\n",
    "\n",
    "    # ðŸ”µ Moving Averages (SMA)\n",
    "    df[\"sma_20\"] = df.groupby(\"symbol\", group_keys=False)[\"close\"].apply(lambda x: SMAIndicator(x, window=20).sma_indicator()).reset_index(level=0, drop=True)\n",
    "    df[\"sma_50\"] = df.groupby(\"symbol\", group_keys=False)[\"close\"].apply(lambda x: SMAIndicator(x, window=50).sma_indicator()).reset_index(level=0, drop=True)\n",
    "\n",
    "    # ðŸŸ¡ MACD Indicator\n",
    "    df[\"macd\"] = df.groupby(\"symbol\", group_keys=False)[\"close\"].apply(lambda x: MACD(x).macd()).reset_index(level=0, drop=True)\n",
    "\n",
    "    # ðŸ”´ Bollinger Bands\n",
    "    df[\"bollinger_upper\"] = df.groupby(\"symbol\", group_keys=False)[\"close\"].apply(lambda x: BollingerBands(x).bollinger_hband()).reset_index(level=0, drop=True)\n",
    "    df[\"bollinger_lower\"] = df.groupby(\"symbol\", group_keys=False)[\"close\"].apply(lambda x: BollingerBands(x).bollinger_lband()).reset_index(level=0, drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = add_indicators(df)\n",
    "\n",
    "# ðŸ§¹ Drop missing values from technical indicators\n",
    "df = df.dropna()\n",
    "\n",
    "# âœ… **Define Features & Target**\n",
    "features = [\"open\", \"high\", \"low\", \"close\", \"volume\", \"trade_count\", \"vwap\", \"rsi\", \"sma_20\", \"sma_50\", \"macd\", \"bollinger_upper\", \"bollinger_lower\"]\n",
    "\n",
    "# âœ… **Set Classification Target (1 = Next Close is Higher, 0 = Lower)**\n",
    "df[\"target\"] = (df.groupby(\"symbol\")[\"close\"].shift(-1) > df[\"close\"]).astype(int)\n",
    "\n",
    "# Drop last row per symbol (since `shift(-1)` creates NaN in last row)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# âœ… **Split Train/Test Per Symbol to Prevent Leakage**\n",
    "train_dfs, test_dfs = [], []\n",
    "for symbol in df[\"symbol\"].unique():\n",
    "    symbol_df = df[df[\"symbol\"] == symbol].sort_values(\"timestamp\")\n",
    "\n",
    "    if len(symbol_df) < 51:  # Ensure enough data for sequences\n",
    "        continue  # Skip symbols with insufficient data\n",
    "\n",
    "    split_idx = int(0.8 * len(symbol_df))  # 80% train, 20% test\n",
    "    train_dfs.append(symbol_df.iloc[:split_idx])\n",
    "    test_dfs.append(symbol_df.iloc[split_idx:])\n",
    "\n",
    "# âœ… **Fit Scaler on Training Data ONLY**\n",
    "all_train_df = pd.concat(train_dfs)  # Combine all training data\n",
    "scaler = MinMaxScaler().fit(all_train_df[features])  # Fit only on training data\n",
    "\n",
    "# âœ… **Scale Train/Test Data**\n",
    "for i in range(len(train_dfs)):\n",
    "    train_dfs[i].loc[:, features] = scaler.transform(train_dfs[i][features])\n",
    "    test_dfs[i].loc[:, features] = scaler.transform(test_dfs[i][features])\n",
    "\n",
    "# ðŸ’¾ **Save Scaler for Later Inference**\n",
    "scaler_path = os.path.join(project_root, \"models\", \"xgboost_scaler.pkl\")\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(\"âœ… Scaler saved successfully!\")\n",
    "\n",
    "# âœ… **Prepare Data for Training**\n",
    "X_train = pd.concat(train_dfs)[features].values\n",
    "y_train = pd.concat(train_dfs)[\"target\"].values\n",
    "\n",
    "X_test = pd.concat(test_dfs)[features].values\n",
    "y_test = pd.concat(test_dfs)[\"target\"].values\n",
    "\n",
    "print(f\"âœ… Training Data: {X_train.shape}, Testing Data: {X_test.shape}\")\n",
    "\n",
    "# ðŸŽ¯ **Train XGBoost Classifier**\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=500,  # More boosting rounds\n",
    "    max_depth=8,  # Higher depth for capturing complex patterns\n",
    "    learning_rate=0.05,  # Lower learning rate\n",
    "    subsample=0.8,  # Uses 80% of data per tree\n",
    "    colsample_bytree=0.8,  # Uses 80% of features per tree\n",
    "    gamma=0.2,  # Reduces overfitting\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# âœ… **Predict Next Price Direction (UP/DOWN)**\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# âœ… **Evaluate Model Accuracy**\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"ðŸ“Š **Model Accuracy:** {accuracy:.4f}\")\n",
    "\n",
    "# ðŸ’¾ **Save Model**\n",
    "models_dir = os.path.join(project_root, \"models\")\n",
    "os.makedirs(models_dir, exist_ok=True)  # Ensure 'models' directory exists\n",
    "model_path = os.path.join(models_dir, \"xgboost_model.pkl\")\n",
    "\n",
    "joblib.dump(model, model_path)\n",
    "print(f\"âœ… Model saved successfully at: {model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
